# Name the components on this agent
 a1.sources = src-1
 a1.sinks = sk-1
 a1.channels = ch-1
#  
#  # Describe/configure the source
  a1.sources.src-1.type = exec
  a1.sources.src-1.shell = /bin/bash -c
  a1.sources.src-1.command = while true; do tm=`date +%s%3N`;name=`hostname`;df=`df | tail -n +2 | awk '{print $5}'| sort -nr| head -n1|awk '{print $1}'`;mem=`free | awk 'FNR == 2 {print $3/($3+$4)*100}'| awk '{printf ("%1.f"),$1}'`;cpu=`top -d 0.5 -b -n2 | grep "Cpu(s)"|tail -n 1 | awk '{print 100-$8}'`;echo "system/diskFileSystem" "timestamp:"$tm,"hostname:"$name,"maxDiskUsage:"$df;echo "system/memory" "timestamp:"$tm,"hostname:"$name,"memUsage:"$mem"%";echo "system/cpuUsage" "timestamp:"$tm,"hostname:"$name,"cpuUsage:"$cpu"%";sleep 1; done;
  #a1.sources.src-1.command = while true; do tm=`date +%s%3N`;name=`hostname`;df=`df | tail -n +2 | awk '{print $5}'| sort -r| head -n1|awk '{print $1}'`;mem=`free | awk 'FNR == 3 {print $3/($3+$4)*100}'| awk '{printf ("%1.f"),$1}'`;cpu=`top -d 0.5 -b -n2 | grep "Cpu(s)"|tail -n 1 | awk '{print $2 + $4}'`;echo "system/diskFileSystem" "log_timestamp:"$tm,"hostname:"$name,"maxDiskUsage:"$df;echo "system/memory" "log_timestamp:"$tm,"hostname:"$name,"memUsage:"$mem"%";echo "system/cpuUsage" "log_timestamp:"$tm,"hostname:"$name,"cpuUsage:"$cpu"%";sleep 5; done;
#
#  #System Interceptor
  a1.sources.src-1.interceptors=i1
  #a1.sources.src-1.interceptors=i1 i2 i3
  a1.sources.src-1.interceptors.i1.type=com.oneapm.log.agent.flume.interceptor.system.SystemUsageInterceptor$SystemUsageInterceptorBuilder
  a1.sources.src-1.interceptors.i1.systemUsageTypes= system/diskFileSystem, system/memory, system/cpuUsage
 # a1.sources.src-1.interceptors.i2.type=com.oneapm.log.agent.flume.interceptor.transform.TransformInterceptor$TransformInterceptorBuilder
  #a1.sources.src-1.interceptors.i3.type=com.oneapm.log.agent.flume.interceptor.transform.TransformInterceptor$TransformInterceptorBuilder
#
#  # Use a channel which buffers events in memory
  a1.channels.ch-1.type = memory
  a1.channels.ch-1.capacity = 100000
  a1.channels.ch-1.transactionCapacity = 10000
#
#  # b) File Roll Sink
#  a1.sinks.sk-1.type = file_roll
#  a1.sinks.sk-1.sink.directory = /app/apache-flume-1.6.0-bin/logs
#  a1.sinks.sk-1.sink.rollInterval = 0

## Sink to kafka cluster
# Kafka sink's type must be defined
 a1.sinks.sk-1.type = org.apache.flume.sink.kafka.KafkaSink
#指定打入的kafka的topic
 a1.sinks.sk-1.topic = mytopic
#制定打入的kafka的brokerlist
 a1.sinks.sk-1.brokerList = 10.45.39.199:9092
#一次可以处理多少条消息
 a1.sinks.sk-1.batchSize = 20
#对于kafka的消息确认级别，1代表只要leader写入消息就返回正确，0代表不等待任何确认，-1代表必须所有的节点都写入数据之后才返回正确
 a1.sinks.sk-1.requiredAcks = 1
#制定sink的源channel是哪个
 a1.sinks.sk-1.channel = ch-1
#
# a1.sinks.sk-1.type = file_roll
# a1.sinks.sk-1.sink.directory = /home/plat/loggs
# a1.sinks.sk-1.sink.rollInterval = 0
# a1.sinks.sk-1.channel = ch-1


## Sink to Kafka
#a1.sinks.sk-1.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.sk-1.topic = mytopic
#a1.sinks.sk-1.brokerList = localhost:9092
#a1.sinks.sk-1.requiredAcks = 1
#a1.sinks.sk-1.batchSize = 20
#a1.sinks.sk-1.channel = c1


#
#  # Bind the source and sink to the channel
  a1.sources.src-1.channels = ch-1
  a1.sinks.sk-1.channel = ch-1 

