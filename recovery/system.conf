# Name the components on this agent
 a1.sources = src-1
 a1.sinks = sk-1
 a1.channels = ch-1
#  
#  # Describe/configure the source
  a1.sources.src-1.type = exec
  a1.sources.src-1.shell = /bin/bash -c
  a1.sources.src-1.command = while true; do tm=`date +%s%3N`;name=`hostname`;df=`df | tail -n +2 | awk '{print $5}'| sort -r| head -n1|awk '{print $1}'`;mem=`free | awk 'FNR == 2 {print $3/($3+$4)*100}'| awk '{printf ("%4.2f"),$1}'`;cpu=`top -d 0.5 -b -n2 | grep "Cpu(s)"|tail -n 1 | awk '{print $2 + $4}'`;echo "system/diskFileSystem" "timestamp:"$tm,"hostname:"$name,"maxDiskUsage:"$df;echo "system/memory" "timestamp:"$tm,"hostname:"$name,"memUsage:"$mem"";echo "system/cpuUsage" "timestamp:"$tm,"hostname:"$name,"cpuUsage:"$cpu"";sleep 5; done;
  #a1.sources.src-1.command = while true; do tm=`date +%s%3N`;name=`hostname`;df=`df | tail -n +2 | awk '{print $5}'| sort -r| head -n1|awk '{print $1}'`;mem=`free | awk 'FNR == 3 {print $3/($3+$4)*100}'| awk '{printf ("%1.f"),$1}'`;cpu=`top -d 0.5 -b -n2 | grep "Cpu(s)"|tail -n 1 | awk '{print $2 + $4}'`;echo "system/diskFileSystem" "log_timestamp:"$tm,"hostname:"$name,"maxDiskUsage:"$df;echo "system/memory" "log_timestamp:"$tm,"hostname:"$name,"memUsage:"$mem"%";echo "system/cpuUsage" "log_timestamp:"$tm,"hostname:"$name,"cpuUsage:"$cpu"%";sleep 5; done;
#
#  #System Interceptor
  a1.sources.src-1.interceptors=i1
  #a1.sources.src-1.interceptors=i1 i2 i3
  a1.sources.src-1.interceptors.i1.type=com.oneapm.log.agent.flume.interceptor.system.SystemUsageInterceptor$SystemUsageInterceptorBuilder
  a1.sources.src-1.interceptors.i1.systemUsageTypes= system/diskFileSystem, system/memory, system/cpuUsage
  #a1.sources.src-1.interceptors.i2.type=com.oneapm.log.agent.flume.interceptor.transform.TransformInterceptor$TransformInterceptorBuilder
  #a1.sources.src-1.interceptors.i3.type=com.oneapm.log.agent.flume.interceptor.transform.TransformInterceptor$TransformInterceptorBuilder
#
#  # Use a channel which buffers events in memory
  a1.channels.ch-1.type = memory
  a1.channels.ch-1.capacity = 100000
  a1.channels.ch-1.transactionCapacity = 10000
#
#  # b) File Roll Sink
#  a1.sinks.sk-1.type = file_roll
#  a1.sinks.sk-1.sink.directory = /app/apache-flume-1.6.0-bin/logs
#  a1.sinks.sk-1.sink.rollInterval = 0

## Sink to ES1 cluster
# Each sink's type must be defined
 a1.sinks.sk-1.type = elasticsearch
# # 客户端方式，默认为transport，端口要设置为9300，如果设置为rest，则需要设置9200端口
 a1.sinks.sk-1.client = transport
# # es集群地址ip[:port][,ip[:port]...]
 a1.sinks.sk-1.hostNames = 10.45.39.191:9300
# # 索引名称前缀，默认flume
 a1.sinks.sk-1.indexName = agent_7_index
# # 索引类型前缀，默认logs
 a1.sinks.sk-1.indexType = agent_7_type
# # 集群名称，默认elasticsearch
 a1.sinks.sk-1.clusterName = xintest
# # 单个事务批量文档数，默认100
 a1.sinks.sk-1.batchSize = 10
# # bulk失败重试次数，默认3，不能<0，0表示不重试
 a1.sinks.sk-1.retryTimes = 3
# # 默认org.apache.flume.sink.elasticsearch.TimeBasedIndexNameBuilder
 a1.sinks.sk-1.indexNameBuilder = org.apache.flume.sink.elasticsearch.TimeBasedIndexNameBuilder
# # 时间格式，默认yyyy-MM-dd
 a1.sinks.sk-1.indexNameBuilder.dateFormat = yyyy-MM-dd
# # 时区，默认UTC
 a1.sinks.sk-1.indexNameBuilder.timeZone = UTC
 # 文档超时，不能设置小于0数值，要么就不设置
a1.sinks.sk-1.ttl = 3d
 # 序列化器，默认org.apache.flume.sink.elasticsearch.ElasticSearchDynamicSerializer
 a1.sinks.sk-1.serializer = org.apache.flume.sink.elasticsearch.serializer.ElasticSearchSourceSerializer


## Sink to Kafka
#a1.sinks.sk1.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.sk1.topic = mytopic
#a1.sinks.sk1.brokerList = localhost:9092
#a1.sinks.sk1.requiredAcks = 1
#a1.sinks.sk1.batchSize = 20
#a1.sinks.sk1.channel = c1


#
#  # Bind the source and sink to the channel
  a1.sources.src-1.channels = ch-1
  a1.sinks.sk-1.channel = ch-1 

